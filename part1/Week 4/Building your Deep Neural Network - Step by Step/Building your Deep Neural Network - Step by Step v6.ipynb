{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建立自己的深度神经网络\n",
    "\n",
    "**变量解析**:\n",
    "-  $[l]$ 表示 $l^{th}$ 层. \n",
    "    - 例如: $a^{[L]}$ 是 $L^{th}$ 层激活函数. $W^{[L]}$ 和 $b^{[L]}$ 是 $L^{th}$ 层参数.\n",
    "-  $(i)$ 表示 $i^{th}$ 样本. \n",
    "    - 例如: $x^{(i)}$ 是 $i^{th}$ 训练样本.\n",
    "- 下划线 $i$ denotes the $i^{th}$ entry of a vector.\n",
    "    - Example: $a^{[l]}_i$ denotes the $i^{th}$ entry of the $l^{th}$ layer's activations).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 -导入所需包\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from testCases_v3 import *\n",
    "from dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2 - 任务提纲\n",
    "\n",
    "- 初始化参数（两层神经网络和L层神经网络）\n",
    "- 实现前向传播\n",
    "     - 实现一层前向传播的线性部分 (结果是 $Z^{[l]}$).\n",
    "     - 激励函数已给出 (relu/sigmoid).\n",
    "     - 结合前面两步形成新的 [LINEAR->ACTIVATION] 前向传播.\n",
    "     - 从1到L层，叠加 [LINEAR->RELU] 函数 L-1 次 并且在L层加上 [LINEAR->SIGMOID] 函数.形成了一个新的L层前向传播函数.\n",
    "- 实现代价函数.\n",
    "- 实现反向传播 .\n",
    "    - 实现一层反向传播的线性部分.\n",
    "    - 激励函数已给出 (relu_backward/sigmoid_backward) \n",
    "    - 结合前面两步形成新的 [LINEAR->ACTIVATION] 反向传播.\n",
    "    - 叠加[LINEAR->RELU] L-1次 并且L层加上 [LINEAR->SIGMOID]函数. 形成了一个新的L层反向反向函数.\n",
    "- 最终更新参数.\n",
    "\n",
    "<img src=\"images/final outline.png\" style=\"width:800px;height:500px;\">\n",
    "<caption><center> **Figure 1**</center></caption><br>\n",
    "\n",
    "\n",
    "**注意** 每一次前向传播函数，都与反向传播函数有联系，在每次前向传播函数中存放在 cache 里面的变量在反向传播函数中用来计算梯度."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3 - 初始化\n",
    "\n",
    "1）初始化两层函数\n",
    "\n",
    "2）初始化L层函数\n",
    "### 3.1 - 两层函数\n",
    "\n",
    "\n",
    "**说明**:\n",
    "- 模型结构为: *LINEAR -> RELU -> LINEAR -> SIGMOID*. \n",
    "- 权重初始化： np.random.randn(shape)*0.01.\n",
    "- 偏置初始化： np.zeros(shape)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    W1 = np.random.randn(n_h,n_x) * 0.01\n",
    "    b1 = np.zeros((n_h,1))\n",
    "    W2 = np.random.randn(n_y,n_h) * 0.01\n",
    "    b2 = np.zeros((n_y,1))\n",
    "    \n",
    "    params = {\"W1\":W1,\n",
    "              \"b1\":b1,\n",
    "              \"W2\":W2,\n",
    "              \"b2\":b2}\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.01624345 -0.00611756 -0.00528172]\n",
      " [-0.01072969  0.00865408 -0.02301539]]\n",
      "b1 = [[0.]\n",
      " [0.]]\n",
      "W2 = [[ 0.01744812 -0.00761207]]\n",
      "b2 = [[0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters(3,2,1)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.2 -L层神经网络\n",
    "\n",
    "当我们用python计算 $W X + b$ , 他将实现广播，例如: \n",
    "\n",
    "$$ W = \\begin{bmatrix}\n",
    "    j  & k  & l\\\\\n",
    "    m  & n & o \\\\\n",
    "    p  & q & r \n",
    "\\end{bmatrix}\\;\\;\\; X = \\begin{bmatrix}\n",
    "    a  & b  & c\\\\\n",
    "    d  & e & f \\\\\n",
    "    g  & h & i \n",
    "\\end{bmatrix} \\;\\;\\; b =\\begin{bmatrix}\n",
    "    s  \\\\\n",
    "    t  \\\\\n",
    "    u\n",
    "\\end{bmatrix}\\tag{2}$$\n",
    "\n",
    " $WX + b$ :\n",
    "\n",
    "$$ WX + b = \\begin{bmatrix}\n",
    "    (ja + kd + lg) + s  & (jb + ke + lh) + s  & (jc + kf + li)+ s\\\\\n",
    "    (ma + nd + og) + t & (mb + ne + oh) + t & (mc + nf + oi) + t\\\\\n",
    "    (pa + qd + rg) + u & (pb + qe + rh) + u & (pc + qf + ri)+ u\n",
    "\\end{bmatrix}\\tag{3}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**说明**:\n",
    "- 模型结构为： *[LINEAR -> RELU] $ \\times$ (L-1) -> LINEAR -> SIGMOID*. I.e.\n",
    "- 前面 $L-1$ 隐藏层激活函数用relu，输出层用sigmoid函数.\n",
    "- 权重初始化：np.random.rand(shape) * 0.01.\n",
    "- 偏置初始化：np.zeros(shape).\n",
    "- 我们将存储 $n^{[l]}$, 在不同层的单元数, 即：layer_dims. \n",
    "- 例如： `layer_dims` ：[2,4,1]: 表示两个输入，一个隐藏，一个输出.\n",
    "- 即`W1` ：(4,2),  `b1` ：(4,1),  `W2` ：(1,4) , `b2` : (1,1). \n",
    "- 下面是一层时,W、b的初始化：\n",
    "```python\n",
    "    if L == 1:\n",
    "        parameters[\"W\" + str(L)] = np.random.randn(layer_dims[1], layer_dims[0]) * 0.01\n",
    "        parameters[\"b\" + str(L)] = np.zeros((layer_dims[1], 1))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    np.random.seed(3)\n",
    "    \n",
    "    params = {}\n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    #str(i) 表示W1,W2后面接着的数字1，2\n",
    "    for i in range(1,L):\n",
    "        params[\"W\" + str(i)] = np.random.randn(layer_dims[i],layer_dims[i-1]) * 0.01\n",
    "        params[\"b\" + str(i)] = np.zeros((layer_dims[i],1))\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n",
      " [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n",
      " [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n",
      " [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[-0.01185047 -0.0020565   0.01486148  0.00236716]\n",
      " [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n",
      " [-0.00768836 -0.00230031  0.00745056  0.01976111]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters_deep([5,4,3])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - 前向传播\n",
    "\n",
    "### 4.1 - 线性前向\n",
    "\n",
    "你将实现下面三个函数：\n",
    "\n",
    "- LINEAR\n",
    "- LINEAR -> ACTIVATION where ACTIVATION will be either ReLU or Sigmoid. \n",
    "- [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID (whole model)\n",
    "\n",
    "线性前向用下列等式计算:\n",
    "\n",
    "$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\tag{4}$$  \n",
    "（$A^{[0]} = X$）\n",
    "\n",
    "\n",
    "**提醒**:\n",
    "上述公式你可以用 np.dot()来表示."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#实现一层前向传播\n",
    "def linear_forward(A, W, b):\n",
    "\n",
    "    Z = np.dot(W,A) + b\n",
    "    \n",
    "    #assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = [[ 3.26295337 -1.23429987]]\n",
      "(array([[ 1.62434536, -0.61175641],\n",
      "       [-0.52817175, -1.07296862],\n",
      "       [ 0.86540763, -2.3015387 ]]), array([[ 1.74481176, -0.7612069 ,  0.3190391 ]]), array([[-0.24937038]]))\n"
     ]
    }
   ],
   "source": [
    "A, W, b = linear_forward_test_case()\n",
    "#这次的输出，下一次的输入\n",
    "#cache 里面包含的是A、 W、 b每一个变量的值\n",
    "Z, linear_cache = linear_forward(A, W, b)\n",
    "print(\"Z = \" + str(Z))\n",
    "print(linear_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 -线性前向激活函数\n",
    "\n",
    "\n",
    "- **Sigmoid**: $\\sigma(Z) = \\sigma(W A + b) = \\frac{1}{ 1 + e^{-(W A + b)}}$ ，表述如下： \n",
    "``` python\n",
    "A, activation_cache = sigmoid(Z)\n",
    "```\n",
    "\n",
    "- **ReLU**:  $A = RELU(Z) = max(0, Z)$ ，表述如下： \n",
    "``` python\n",
    "A, activation_cache = relu(Z)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**提醒**: 实现线性前向函数 *LINEAR->ACTIVATION* 层. 即: \n",
    "\n",
    "$A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})$\n",
    "\n",
    "激活函数 \"g\" 可以是 sigmoid() 或者 relu()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \n",
    "    if activation == 'sigmoid':\n",
    "        Z,linear_cache = linear_forward(A_prev, W, b)\n",
    "        A,activation_cache = sigmoid(Z)\n",
    "        \n",
    "    elif activation == 'relu':\n",
    "        Z,linear_cache = linear_forward(A_prev, W, b)\n",
    "        A,activation_cache = relu(Z)\n",
    "        \n",
    "    cache = (linear_cache, activation_cache)\n",
    "    \n",
    "    return A,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With sigmoid: A = [[0.96890023 0.11013289]]\n",
      "With ReLU: A = [[3.43896131 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "A_prev, W, b = linear_activation_forward_test_case()\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n",
    "print(\"With sigmoid: A = \" + str(A))\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n",
    "print(\"With ReLU: A = \" + str(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意**: 在深度学习中， \"[LINEAR->ACTIVATION]\" 在神经网络里面被看作一层，而不是两层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d)L层模型\n",
    "\n",
    "<img src=\"images/model_architecture_kiank.png\" style=\"width:600px;height:300px;\">\n",
    "<caption><center> **Figure 2** : *[LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID* model</center></caption><br>\n",
    "\n",
    "\n",
    "**Instruction**:  $A^{[L]} = \\sigma(Z^{[L]}) = \\sigma(W^{[L]} A^{[L-1]} + b^{[L]})$. (有时叫做 `Yhat`, i.e.,即$\\hat{Y}$.) \n",
    "\n",
    "**技巧**:\n",
    "- 用之前你已经写过的函数 \n",
    "- 用一个for循环来迭代L-1次 [LINEAR->RELU] \n",
    "- 别忘了跟踪“缓存”列表中的缓存. 可以用 `list.append(cache)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  当 A0 = X 这样赋值时，会出现 ValueError: shapes (3,4) and (5,4) not aligned: 4 (dim 1) != 5 (dim 0),\n",
    "#  A_prev = A 是每次迭代都需要更新的值\n",
    "\n",
    "#params[\"W\" + str(i)]：for循环每次的i体现在W后面接着的数字变化，即i为变换的层数\n",
    "#list.append(cache):list表示变量，即list是caches，而不是要caches = list.append(cache)\n",
    "def L_model_forward(X, params):\n",
    "   \n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(params) // 2 #在神经网络中的层数\n",
    "    \n",
    "    for i in range(1, L):\n",
    "        \n",
    "        A_prev = A\n",
    "        A, cache = linear_activation_forward(A_prev, params[\"W\" + str(i)], params[\"b\" + str(i)], activation = \"relu\")\n",
    "        caches.append(cache)\n",
    "        \n",
    "    AL,cache = linear_activation_forward(A, params[\"W\" + str(L)], params[\"b\" + str(L)], activation = \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    return AL,caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL = [[0.03921668 0.70498921 0.19734387 0.04728177]]\n",
      "Length of caches list = 3\n"
     ]
    }
   ],
   "source": [
    "X, parameters = L_model_forward_test_case_2hidden()\n",
    "AL, caches = L_model_forward(X, parameters)\n",
    "print(\"AL = \" + str(AL))\n",
    "print(\"Length of caches list = \" + str(len(caches)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - 计算代价函数\n",
    "\n",
    "**说明**: \n",
    "\n",
    "1）计算交叉熵成本 $J$\n",
    "\n",
    "2）用下列式子: $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right)) \\tag{7}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#每一次函数定义的参数要知道含义：AL:预测的Yhat, Y:实际值\n",
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    #第一种\n",
    "    J = -np.sum((Y * np.log(AL)) + (1 - Y) * np.log(1 - AL)) /m\n",
    "    #第二种：将 * 用np.multiply代替，其余不变\n",
    "#     logprosp = np.multiply(Y, np.log(AL))\n",
    "#     logprosp_1 = np.multiply((1 - Y), np.log(1 - AL))\n",
    "#     J = -np.sum(logprosp+logprosp_1) / m\n",
    "    \n",
    "    cost = np.squeeze(J)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 0.414931599615397\n"
     ]
    }
   ],
   "source": [
    "Y, AL = compute_cost_test_case()\n",
    "\n",
    "print(\"cost = \" + str(compute_cost(AL, Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - 反向传播模型\n",
    "\n",
    "**提醒**: \n",
    "<img src=\"images/backprop_kiank.png\" style=\"width:650px;height:250px;\">\n",
    "<caption><center> **Figure 3** : 前向和反向传播模型: *LINEAR->RELU->LINEAR->SIGMOID* <br> *紫色代表前向传播，红色代表反向传播.*  </center></caption>\n",
    "\n",
    "<!-- \n",
    "For those of you who are expert in calculus (you don't need to be to do this assignment), the chain rule of calculus can be used to derive the derivative of the loss $\\mathcal{L}$ with respect to $z^{[1]}$ in a 2-layer network as follows:\n",
    "\n",
    "$$\\frac{d \\mathcal{L}(a^{[2]},y)}{{dz^{[1]}}} = \\frac{d\\mathcal{L}(a^{[2]},y)}{{da^{[2]}}}\\frac{{da^{[2]}}}{{dz^{[2]}}}\\frac{{dz^{[2]}}}{{da^{[1]}}}\\frac{{da^{[1]}}}{{dz^{[1]}}} \\tag{8} $$\n",
    "\n",
    "In order to calculate the gradient $dW^{[1]} = \\frac{\\partial L}{\\partial W^{[1]}}$, you use the previous chain rule and you do $dW^{[1]} = dz^{[1]} \\times \\frac{\\partial z^{[1]} }{\\partial W^{[1]}}$. During the backpropagation, at each step you multiply your current gradient by the gradient corresponding to the specific layer to get the gradient you wanted.\n",
    "\n",
    "Equivalently, in order to calculate the gradient $db^{[1]} = \\frac{\\partial L}{\\partial b^{[1]}}$, you use the previous chain rule and you do $db^{[1]} = dz^{[1]} \\times \\frac{\\partial z^{[1]} }{\\partial b^{[1]}}$.\n",
    "\n",
    "This is why we talk about **backpropagation**.\n",
    "!-->\n",
    "\n",
    "反向传播三步走:\n",
    "- LINEAR backward\n",
    "- LINEAR -> ACTIVATION backward where ACTIVATION computes the derivative of either the ReLU or sigmoid activation\n",
    "- [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID backward (whole model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 - 线性反向\n",
    "\n",
    "对于 $l$ 层, 线性部分为: $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$ .\n",
    "\n",
    "已知： $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$.\n",
    "\n",
    "最终： $(dW^{[l]}, db^{[l]} dA^{[l-1]})$.\n",
    "\n",
    "<img src=\"images/linearback_kiank.png\" style=\"width:250px;height:300px;\">\n",
    "<caption><center> **Figure 4** </center></caption>\n",
    "\n",
    "三个输出 $(dW^{[l]}, db^{[l]}, dA^{[l]})$ 是为了 $dZ^{[l]}$的输入.\n",
    "\n",
    "如下三个公式:\n",
    "$$ dW^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\tag{8}$$\n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{9}$$\n",
    "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \\tag{10}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW = np.dot(dZ, A_prev.T) / m\n",
    "    db = np.sum(dZ) / m\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "#     grads = {\"dW\":dW,\n",
    "#              \"db\":db,\n",
    "#              \"dA_prev\":dA_prev}\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_prev = [[ 0.51822968 -0.19517421]\n",
      " [-0.40506361  0.15255393]\n",
      " [ 2.37496825 -0.89445391]]\n",
      "dW = [[-0.10076895  1.40685096  1.64992505]]\n",
      "db = 0.5062944750065832\n"
     ]
    }
   ],
   "source": [
    "# Set up some test inputs\n",
    "dZ, linear_cache = linear_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 - 线性反向激励函数\n",
    "\n",
    "为帮助实现`linear_activation_backward`函数, 提供两个反向函数:\n",
    "- **`sigmoid_backward`**:\n",
    "\n",
    "```python\n",
    "dZ = sigmoid_backward(dA, activation_cache)\n",
    "```\n",
    "\n",
    "- **`relu_backward`**: \n",
    "\n",
    "```python\n",
    "dZ = relu_backward(dA, activation_cache)\n",
    "```\n",
    "\n",
    "如果 $g(.)$ 是激励函数, \n",
    "`sigmoid_backward`和 `relu_backward` 来计算 $$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]}) \\tag{11}$$.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \n",
    "    linear_cache,activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ,linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ,linear_cache)\n",
    "        \n",
    "    return dA_prev, dW, db\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid:\n",
      "dA_prev = [[ 0.11017994  0.01105339]\n",
      " [ 0.09466817  0.00949723]\n",
      " [-0.05743092 -0.00576154]]\n",
      "dW = [[ 0.10266786  0.09778551 -0.01968084]]\n",
      "db = -0.057296222176291135\n",
      "\n",
      "relu:\n",
      "dA_prev = [[ 0.44090989  0.        ]\n",
      " [ 0.37883606  0.        ]\n",
      " [-0.2298228   0.        ]]\n",
      "dW = [[ 0.44513824  0.37371418 -0.10478989]]\n",
      "db = -0.2083789237027353\n"
     ]
    }
   ],
   "source": [
    "AL, linear_activation_cache = linear_activation_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(AL, linear_activation_cache, activation = \"sigmoid\")\n",
    "print (\"sigmoid:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db) + \"\\n\")\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(AL, linear_activation_cache, activation = \"relu\")\n",
    "print (\"relu:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 - L层反向模型\n",
    "cache --> [X,W,b,Z]\n",
    "在L_model_forward模型里面你将会用cache里面的变量计算梯度，且从第L层开始迭代所有的隐藏层\n",
    "\n",
    "<img src=\"images/mn_backward.png\" style=\"width:450px;height:300px;\">\n",
    "<caption><center>  **Figure 5** : Backward pass  </center></caption>\n",
    "\n",
    "** 初始化反向传播**:\n",
    "输出： \n",
    "$A^{[L]} = \\sigma(Z^{[L]})$. Your code thus needs to compute `dAL` $= \\frac{\\partial \\mathcal{L}}{\\partial A^{[L]}}$.\n",
    "\n",
    "```python\n",
    "dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) \n",
    "```\n",
    "grads --> [dA、dW、db]\n",
    "\n",
    "dAL   --> LINEAR->SIGMOID backward\n",
    "\n",
    "for循环 --> LINEAR->RELU backward \n",
    "\n",
    "$$grads[\"dW\" + str(l)] = dW^{[l]}\\tag{15} $$\n",
    "\n",
    "对于l = 3 : $dW^{[l]}$ --> `grads[\"dW3\"]`.\n",
    "\n",
    "**实现反向传播模型**： *[LINEAR->RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID* ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    \n",
    "    #grads[\"dA\" + str(L-1)] = dAL-1、grads[\"dW\" + str(L)] = dWL、grads[\"db\" + str(L)] = dbL\n",
    "    #即下列式子为 dAL-1（dA_prev) ,dWL, dbL = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
    "    #充分运用之前的函数,误区：grads[\"dA\" + str(L-1)],grads[\"dW\" + str(L)],grads[\"db\" + str(L)] = dAL-1,dWL,dbL\n",
    "    #L = 1,2,.....L\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y,1 - AL))\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)],grads[\"dW\" + str(L)],grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
    "    \n",
    "    #grads[\"dA\" + str(i+1)] : i=0相当于从dA1算起，则str(i +1)\n",
    "    #for i in range(1,L-1):\n",
    "    for i in reversed(range(L-1)):\n",
    "        current_cache = caches[i]\n",
    "        dA_prev, dW, db = linear_activation_backward(grads[\"dA\" + str(i+1)], current_cache, activation = \"relu\")\n",
    "        grads[\"dA\" + str(i)] = dA_prev\n",
    "        grads[\"dW\" + str(i + 1)] = dW\n",
    "        grads[\"db\" + str(i + 1)] = db\n",
    "        \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "dW1 = [[0.41010002 0.07807203 0.13798444 0.10502167]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.05283652 0.01005865 0.01777766 0.0135308 ]]\n",
      "db1 = -0.24842412164385988\n",
      "dA1 = [[ 0.12913162 -0.44014127]\n",
      " [-0.14175655  0.48317296]\n",
      " [ 0.01663708 -0.05670698]]\n"
     ]
    }
   ],
   "source": [
    "AL, Y_assess, caches = L_model_backward_test_case()\n",
    "grads = L_model_backward(AL, Y_assess, caches)\n",
    "print_grads(grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 - 更新参数\n",
    " \n",
    "\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{16}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{17}$$\n",
    "\n",
    "$\\alpha$ 是学习率. 更新完参数后，放在params里面. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(params, grads, learning_rate):\n",
    "    \n",
    "    L = len(params) // 2 #神经网络层数\n",
    "    \n",
    "    for i in range(L):\n",
    "        params[\"W\" + str(i + 1)] = params[\"W\" + str(i + 1)] - learning_rate * grads[\"dW\" + str(i + 1)]\n",
    "        params[\"b\" + str(i + 1)] = params[\"b\" + str(i + 1)] - learning_rate * grads[\"db\" + str(i + 1)]\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
      " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
      " [-1.0535704  -0.86128581  0.68284052  2.20374577]]\n",
      "b1 = [[-0.04659241]\n",
      " [-1.28888275]\n",
      " [ 0.53405496]]\n",
      "W2 = [[-0.55569196  0.0354055   1.32964895]]\n",
      "b2 = [[-0.84610769]]\n"
     ]
    }
   ],
   "source": [
    "parameters, grads = update_parameters_test_case()\n",
    "parameters = update_parameters(parameters, grads, 0.1)\n",
    "\n",
    "print (\"W1 = \"+ str(parameters[\"W1\"]))\n",
    "print (\"b1 = \"+ str(parameters[\"b1\"]))\n",
    "print (\"W2 = \"+ str(parameters[\"W2\"]))\n",
    "print (\"b2 = \"+ str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 7 - 总结\n",
    "\n",
    "#Steps:\n",
    "\n",
    "1、 Parameters initialization\n",
    "\n",
    "2、 Loop (gradient descent)\n",
    "\n",
    "3、 Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "\n",
    "4、 Compute cost.\n",
    "\n",
    "5、 Backward propagation.\n",
    "\n",
    "6、 Update parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693049735659989\n",
      "Cost after iteration 100: 0.6464320953428849\n",
      "Cost after iteration 200: 0.6325140647912678\n",
      "Cost after iteration 300: 0.6015024920354665\n",
      "Cost after iteration 400: 0.5601966311605748\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6555023923444976\n",
      "Accuracy: 0.3400000000000001\n",
      "Cost after iteration 0: 0.771749\n",
      "Cost after iteration 100: 0.672053\n",
      "Cost after iteration 200: 0.648263\n",
      "Cost after iteration 300: 0.611507\n",
      "Cost after iteration 400: 0.567047\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from dnn_app_utils_v2 import *\n",
    "import imageio\n",
    "import scipy.misc\n",
    "\n",
    "#two_layer_model\n",
    " \n",
    "def two_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    " \n",
    "    np.random.seed(1)\n",
    "    grads = {}\n",
    "    costs = []                              \n",
    "    m = X.shape[1]                          \n",
    "    (n_x, n_h, n_y) = layers_dims\n",
    " \n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    " \n",
    " \n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        A1, cache1 = linear_activation_forward(X, W1, b1, activation=\"relu\")\n",
    "        A2, cache2 = linear_activation_forward(A1, W2, b2, activation=\"sigmoid\")\n",
    "\n",
    "        cost = compute_cost(A2, Y)\n",
    "\n",
    "        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n",
    " \n",
    "        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, activation=\"sigmoid\")\n",
    "        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, activation=\"relu\")\n",
    "\n",
    "        grads['dW1'] = dW1\n",
    "        grads['db1'] = db1\n",
    "        grads['dW2'] = dW2\n",
    "        grads['db2'] = db2\n",
    " \n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    " \n",
    "        W1 = parameters[\"W1\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    " \n",
    "\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    " \n",
    "    # plot the cost\n",
    " \n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    " \n",
    "    return parameters\n",
    "\n",
    "\n",
    "\n",
    "# L_layer_model\n",
    "def L_layer_model(X, Y, layers_dims, learning_rate=0.0075, num_iterations=3000, print_cost=False):  # lr was 0.009\n",
    "\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []  # keep track of cost\n",
    "\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "   \n",
    "    for i in range(0, num_iterations):\n",
    "        \n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "\n",
    "        cost = compute_cost(AL, Y)\n",
    "\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration %i: %f\" % (i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "\n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    return parameters\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    aa = time.time()\n",
    "    np.random.seed(1)\n",
    "    train_x_orig, train_y, test_x_orig, test_y, classes = load_data()\n",
    "    # Reshape the training and test examples\n",
    "    train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0],\n",
    "                                           -1).T  # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "    test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "    \n",
    "    num_px = train_x_orig.shape[1]\n",
    "    \n",
    "    # Standardize data to have feature values between 0 and 1.\n",
    "    train_x = train_x_flatten / 255.\n",
    "    test_x = test_x_flatten / 255.\n",
    "\n",
    "    n_x = 12288  # num_px * num_px * 3\n",
    "    n_h = 7\n",
    "    n_y = 1\n",
    "    layers_dims = (n_x, n_h, n_y)\n",
    "    parameters = two_layer_model(train_x, train_y, layers_dims=(n_x, n_h, n_y), num_iterations=500, print_cost=True)\n",
    "    predictions_train = predict(train_x, train_y, parameters)\n",
    "    predictions_test = predict(test_x, test_y, parameters)\n",
    "\n",
    "    layers_dims_L = [12288, 20, 7, 5, 1]  # 5-layer model\n",
    "    parameters = L_layer_model(train_x, train_y, layers_dims_L, num_iterations=500, print_cost=True)\n",
    "    \n",
    "    my_image = \"img.png\"  \n",
    "    my_label_y = [0]  \n",
    "\n",
    "    from skimage.transform import resize\n",
    "    fname = \"image/\" + my_image\n",
    "    image = np.array(imageio.imread(fname))\n",
    "    my_image = resize(image, output_shape=(num_px,num_px)).reshape((1, num_px*num_px*3)).T\n",
    "    #my_image = resize(image, size=(64, 64)).reshape((64 * 64 * 3, 1))\n",
    "    my_predicted_image = predict(my_image, my_label_y, parameters)\n",
    "\n",
    "    plt.imshow(image)\n",
    "    print(\"y = \" + str(np.squeeze(my_predicted_image)) + \", your L-layer model predicts a \\\"\" + classes[\n",
    "        int(np.squeeze(my_predicted_image)),].decode(\"utf-8\") + \"\\\" picture.\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "c4HO0",
   "launcher_item_id": "lSYZM"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
